image: 
resources:
  requests:
    cpu: "300m"
    memory: "256Mi"
  limits:
    cpu: "1000m"
    memory: "2048Mi"
ebs:
  storage: 200Gi
  mount_path: /tmp
inline_script:
- |+
  cat > /tmp/run_accuracy_metric_calculation.py <<EOF
  
  import json
  import os
  from sklearn.metrics import accuracy_score, f1_score

  results = []
  output_prefix = os.environ['OUTPUT_PREFIX']
  results_path = f"{output_prefix}_test_pubmedqa_inputs_preds_labels.jsonl"
  with open(results_path,'rt') as f:
    while st := f.readline():
      results.append(json.loads(st))

  truth = []
  preds = []
  
  for result in results:
    truth.append(result['label'])
    preds.append(result['pred'])

  acc = accuracy_score(truth, preds)
  maf = f1_score(truth, preds, average='macro')

  print('Accuracy %f' % acc)
  print('Macro-F1 %f' % maf)

  EOF
pre_script: 
  - export OUTPUT_PREFIX=$LOG_ROOT/nemo_experiments/$EXP_NAME/eval_results
  - OUTPUT_LOG=$LOG_ROOT/peft_accuracy.log
process:
  env:
    - name: LOG_ROOT
      value: "/efs/home/{{ .Release.Name }}/logs"
    - name: EXP_NAME
      value: "peft_pubmedqa"
  command:
    -  "python"
  args: 
    - /tmp/run_accuracy_metric_calculation.py
    - '2>&1 | tee $OUTPUT_LOG' 
