## Training Tutorials


### Hugging Face Accelerate

* Framework: [Hugging Face Accelerate](https://huggingface.co/docs/accelerate/en/index)
* Accelerator: GPU (CUDA)

| Model      | Description |
| ----------- | ----------- |
| [Bert GLUE MRPC Fine-tuning](./accelerate/bert-glue-mrpc/README.md)     | BERT Glue MRPC     |
| [Llama 2 7B Fine-tuning](./accelerate/llama2-ft-fsdp/README.md)     | Llama 2 7B Fine-tuning with FSDP  |
| [Llama 2 13B Fine-tuning](./accelerate/llama2-ft-fsdp/README.md)     | Llama 2 13B Fine-tuning with FSDP |
| [Llama 2 70B Fine-tuning](./accelerate/llama2-ft-fsdp/README.md)     | Llama 2 70B Fine-tuning with FSDP |

### Megatron DeepSpeed

* Framework: [Megatron DeepSpeed](https://github.com/deepspeedai/Megatron-DeepSpeed)
* Accelerator: GPU (CUDA)


| Model      | Description |
| ----------- | ----------- |
| [GPT2 345M Pre-train](./megatron-deepspeed/gpt2_345m/README.md)     | Pre-train GPT2 345M on Wikicorpus dataset  |

### Nemo

* Framework: [Nemo](https://github.com/NVIDIA/NeMo)
* Accelerator: GPU (CUDA)


| Model      | Description |
| ----------- | ----------- |
| [Llama 2 7B PEFT LoRA](./nemo-megatron/llama2-7b-peft/README.md)     | Llama 2 7B PEFT LoRA on Wikicorpus dataset |
| [Llama 2 7B PEFT LoRA](./nemo-megatron/llama31-8b-peft-dolphin/README.md)     | Llama 2 7B PEFT LoRA on Dolphin dataset |

### Neuronx Distributed

* Framework: [Neuronx Distributed](https://github.com/aws-neuron/neuronx-distributed)
* Accelerator: AWS Trainium


| Model      | Description |
| ----------- | ----------- |
| [GPT Neox 6.9B Pre-train ](./neuronx-distributed/gpt_neox_6.9b/README.md)     | GPT Neox 6.9B pre-train on Wikicorpus |
| [GPT Neox 20B Pre-train ](./neuronx-distributed/gpt_neox_20b/README.md)     | GPT Neox 20B pre-train on Wikicorpus |
| [Llama 2 7B Pre-train ](./neuronx-distributed/llama2_7b/README.md)     | Llama2 7B pre-train on Wikicorpus |
| [Llama 2 7B Pre-train](./neuronx-distributed/llama2_7b_ptl/README.md)     | Llama2 7B pre-train on Wikicorpus with PyTorch Lightning |
| [Llama 2 13B Pre-train ](./neuronx-distributed/llama2_13b/README.md)     | Llama2 13B pre-train on Wikicorpus |
| [Llama 2 13B Pre-train](./neuronx-distributed/llama2_13b_ptl/README.md)     | Llama2 13B pre-train on Wikicorpus with PyTorch Lightning |
| [Llama 2 70B Pre-train ](./neuronx-distributed/llama2_70b/README.md)     | Llama2 70B pre-train on Wikicorpus |
| [Llama 2 70B Pre-train](./neuronx-distributed/llama2_70b_ptl/README.md)     | Llama2 70B pre-train on Wikicorpus with PyTorch Lightning |
| [Llama 3 8B Pre-train ](./neuronx-distributed/llama3_8b/README.md)     | Llama3 8B pre-train on Wikicorpus |
| [Llama 3 70B Pre-train ](./neuronx-distributed/llama3_70b/README.md)     | Llama3 70B pre-train on Wikicorpus |
| [Llama 3 70B Pre-train ](./neuronx-distributed/llama3_70b_ptl/README.md)     | Llama3 70B pre-train on Wikicorpus with PyTorch Lightning |
| [Llama 3.1 8B Pre-train ](./neuronx-distributed/llama31_8b/README.md)     | Llama3.1 8B pre-train on Wikicorpus |
| [Llama 3.1 70B Pre-train ](./neuronx-distributed/llama31_70b/README.md)     | Llama3.1 70B pre-train on Wikicorpus |

### Neuronx Distributed Training

* Framework: [Neuronx Distributed Training](https://github.com/aws-neuron/neuronx-distributed-training)
* Accelerator: AWS Trainium

| Model      | Description |
| ----------- | ----------- |
| [Llama 3 70B ](./neuronx-distributed-training/llama3_70b/README.md)     | Llama3 70B pre-train on Wikicorpus |

### RayTrain 

* Framework: [RayTrain](https://docs.ray.io/en/latest/train/train.html)
* Accelerator: GPU (CUDA)


| Model      | Description |
| ----------- | ----------- |
| [BERT](./raytrain/lightning-bert/README.md)     | Fine-tune BERT  using Lightning|






