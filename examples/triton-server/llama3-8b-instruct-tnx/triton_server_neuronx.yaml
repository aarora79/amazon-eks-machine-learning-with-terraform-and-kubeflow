image: 
  name:
  pull_policy: Always
scheduler_name: neuron-scheduler
resources:
  node_type: inf2.48xlarge
  resources:
  requests:
    "aws.amazon.com/neuron": 4
  limits:
    "aws.amazon.com/neuron": 4
tolerations:
  - key: "aws.amazon.com/neuron"
    operator: "Exists"
    effect: "NoSchedule"
ebs:
  storage: 400Gi
  mount_path: /tmp
inline_script:
- |+
  cat > /tmp/model.py <<EOF
  import json
  import os

  import torch
  from transformers_neuronx import NeuronAutoModelForCausalLM
  from transformers import AutoTokenizer
  import numpy as np

  import triton_python_backend_utils as pb_utils

  _MODEL_ARGS_FILENAME = "model.json"
  _MAX_MODEL_LEN = 8192

  class TritonPythonModel:

      def initialize(self, args):

        self.model_config = json.loads(args["model_config"])
        text_output_config = pb_utils.get_output_config_by_name(self.model_config, "text_output")
        self.text_output_dtype = pb_utils.triton_string_to_numpy(text_output_config["data_type"])
        self.load_model()
        

      @staticmethod
      def auto_complete_config(auto_complete_model_config):
          
        inputs = [
          {"name": "text_input", "data_type": "TYPE_STRING", "dims": [1]},
          {
              "name": "sampling_parameters",
              "data_type": "TYPE_STRING",
              "dims": [1],
              "optional": True,
          }
        ]
        outputs = [{"name": "text_output", "data_type": "TYPE_STRING", "dims": [-1]}]

        config = auto_complete_model_config.as_dict()
        input_names = []
        output_names = []
        for input in config['input']:
            input_names.append(input['name'])
        for output in config['output']:
            output_names.append(output['name'])

        for input in inputs:
            if input['name'] not in input_names:
                auto_complete_model_config.add_input(input)
        for output in outputs:
            if output['name'] not in output_names:
                auto_complete_model_config.add_output(output)

        auto_complete_model_config.set_model_transaction_policy(dict(decoupled=False))
        auto_complete_model_config.set_max_batch_size(1)
        auto_complete_model_config.set_dynamic_batching()

        return auto_complete_model_config
      
      def load_model(self):
        print("Enter: load_model")

        max_batch_size = int(self.model_config.get('max_batch_size', 1))
        assert (
            max_batch_size >= 1 
        ), "max_batch_size must be >= 1 for dynamic batching"

        self.using_decoupled = pb_utils.using_decoupled_model_transaction_policy(self.model_config) 
        assert (
            not self.using_decoupled 
        ), "Python backend must be configured to not use decoupled model transaction policy"

        model_args_filepath = os.path.join( 
            pb_utils.get_model_dir(), _MODEL_ARGS_FILENAME
        )
        assert os.path.isfile(
            model_args_filepath
        ), f"'{_MODEL_ARGS_FILENAME}' containing model args must be provided in '{pb_utils.get_model_dir()}'"
        with open(model_args_filepath) as file:
            properties = json.load(file)

        model_location = properties.get("model")
        amp=properties.get("amp", "f16")
        tp_degree = int(properties.get("tensor_parallel_degree", 8))
        self.n_positions = int(properties.get("n_positions", _MAX_MODEL_LEN))

        tokenizer_location = properties.get("tokenizer")
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_location)
        self.model = NeuronAutoModelForCausalLM.from_pretrained(model_location,
                                                        batch_size=1,
                                                        n_positions=self.n_positions, 
                                                        tp_degree=tp_degree, 
                                                        amp=amp)

        print(f"Move model to Neuron device")
        self.model.to_neuron()

        print("Exit: load_model")


      def get_sampling_params_dict(self, params_json):              
        params_dict = json.loads(params_json)

        float_keys = [
            "temperature",
            "top_p"
        ]
        for k in float_keys:
            if k in params_dict:
                params_dict[k] = float(params_dict[k])
            
        int_keys = ["sequence_length", "top_k"]
        for k in int_keys:
            if k in params_dict:
                params_dict[k] = int(params_dict[k])

        if not params_dict:
          params_dict["sequence_length"] = _MAX_MODEL_LEN
          params_dict["top_k"] = 50
        elif "sequence_length" not in params_dict:
          params_dict["sequence_length"] = _MAX_MODEL_LEN

        return params_dict

      def execute(self, requests):
        output_dtype = self.text_output_dtype
  
        responses = []
        for request in requests:
          prompts = pb_utils.get_input_tensor_by_name(request, "text_input").as_numpy().flatten()
          prompts = prompts.tolist()
          prompts = [ p.decode("utf-8") if isinstance(p, bytes) else p for p in prompts]

          parameters_input_tensor = pb_utils.get_input_tensor_by_name(request, "sampling_parameters")
          if parameters_input_tensor:
            parameters = parameters_input_tensor.as_numpy().flatten()
            parameters = parameters.tolist()[0] # assume uniform sampling parameters in batch
            parameters = parameters.decode('utf-8') if isinstance(parameters, bytes) else parameters
          else:
            parameters = request.parameters()

          params = self.get_sampling_params_dict(parameters)

          text_output_seqs = []
          with torch.inference_mode():
            for prompt in prompts:
              input_ids = self.tokenizer.encode_plus(prompt, return_tensors="pt")['input_ids']
              generated_token_seqs = self.model.sample(input_ids, **params)
              print(f"generated_token_seqs: {generated_token_seqs}")
              text_output_seqs.append([self.tokenizer.decode(seq, skip_special_tokens=True) for seq in generated_token_seqs])


          out_tensor = pb_utils.Tensor("text_output", np.array(text_output_seqs).astype(output_dtype))
          inference_response = pb_utils.InferenceResponse(output_tensors=[out_tensor])

          responses.append(inference_response)

        return responses

      def finalize(self):
        print("Cleaning up...")

  EOF

  cat > /tmp/config.pbtxt <<EOF
  backend: "python"
  max_batch_size: 8
  model_transaction_policy {
    decoupled: false
  }

  dynamic_batching {
    max_queue_delay_microseconds: 100
  }

  input [ 
    {
      name: "text_input"
      data_type: TYPE_STRING
      dims: [1]
    },
    {
        name: "sampling_parameters"
        data_type: TYPE_STRING
        dims: [1]
        optional: true
    }
  ] 
  output [
    {
      name: "text_output"
      data_type: TYPE_STRING
      dims: [-1]
    }
  ]

  instance_group [
    {
      count: 1
      kind: KIND_MODEL
    }
  ]
  
  EOF
- |+
  cat > /tmp/model.json <<EOF
  {
    "model": "/fsx/pretrained-models/meta-llama/Meta-Llama-3-8B-Instruct",
    "tokenizer": "/fsx/pretrained-models/meta-llama/Meta-Llama-3-8B-Instruct",
    "tensor_parallel_degree": 8,
    "amp": "f16",
    "n_positions": 8192
  }

  EOF

pre_script: 
  - mkdir -p $LOG_ROOT
  - OUTPUT_LOG="$LOG_ROOT/triton-server.log"
  - mkdir -p $MODEL_REPO
  - VERSION=1
  - MODEL_NAME=llama3-8b-instruct
  - mkdir -p $MODEL_REPO/$MODEL_NAME/$VERSION
  - cp /tmp/model.py $MODEL_REPO/$MODEL_NAME/$VERSION/model.py
  - cp /tmp/model.json $MODEL_REPO/$MODEL_NAME/$VERSION/model.json
  - cp /tmp/config.pbtxt $MODEL_REPO/$MODEL_NAME/config.pbtxt
  - export NEURON_CC_FLAGS="--model-type transformer"
  - export NEURON_COMPILE_CACHE_URL="/tmp"
server:
  ports:
    http: 8000
    grpc: 8001
    metrics: 8002
  readiness_probe:
    period_secs: 30
    failure_threshold: 10
  startup_probe:
    period_secs: 60
    failure_threshold: 30
  liveness_probe:
    period_secs: 30
    failure_threshold: 10
  env:
    - name: HOME
      value: /tmp
    - name: LOG_ROOT
      value: /efs/home/{{ .Release.Name }}/logs
    - name: MODEL_REPO
      value: /efs/home/{{ .Release.Name }}/model_repository
  command:
    - tritonserver
  args:
    - --model-repository=${MODEL_REPO}
    - --grpc-port=8001
    - --http-port=8000
    - --metrics-port=8002
    - --disable-auto-complete-config
    - --log-file=$OUTPUT_LOG
  autoscaling:
    minReplicas: 3
    maxReplicas: 6
    metrics:
      - type: Pods
        pods:
          metric:
            name: avg_time_queue_us
          target:
            type: AverageValue
            averageValue: 1000000
