image: nvcr.io/nvidia/tritonserver:24.05-trtllm-python-py3
resources:
  node_type: g5.48xlarge
  requests:
    "nvidia.com/gpu": 8
  limits:
    "nvidia.com/gpu": 8
ebs:
  storage: 400Gi
  mount_path: /tmp
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
git:
  repo_url: "https://github.com/NVIDIA/TensorRT-LLM.git"
  commit: 250d9c293d5edbc2a45c20775b3150b1eb68b364
  branch: main
pre_script: 
  - mkdir -p $LOG_ROOT
  - TP_SIZE=8
  - PP_SIZE=1
  - OUTPUT_LOG=$LOG_ROOT/build_trtllm_tp_${TP_SIZE}_pp_${PP_SIZE}.log
  - CKPT_PATH=$OUTPUT_ROOT/ckpt_tp_${TP_SIZE}_pp_${PP_SIZE}
  - TMP_ENGINE_DIR=/tmp/engine_tp_${TP_SIZE}_pp_${PP_SIZE}
post_script:
  - mpirun --allow-run-as-root -np $TP_SIZE python3 examples/run.py --tokenizer_dir $MODEL_PATH --engine_dir $TMP_ENGINE_DIR --max_output_len 128
  - rm -rf $OUTPUT_ROOT/engine_tp_${TP_SIZE}_pp_${PP_SIZE}
  - cp -r $TMP_ENGINE_DIR $OUTPUT_ROOT/
process:
  env:
    - name: LOG_ROOT
      value: /efs/home/{{ .Release.Name }}/logs
    - name: OUTPUT_ROOT
      value: /efs/home/{{ .Release.Name }}/trtllm
    - name: MODEL_PATH
      value: /fsx/pretrained-models/mistralai/Mistral-7B-Instruct-v0.1
  command:
    - trtllm-build
  args:
    - --checkpoint_dir ${CKPT_PATH}
    - --max_num_tokens 32768
    - --tp_size ${TP_SIZE}
    - --pp_size ${PP_SIZE}
    - --gpus_per_node 8
    - --remove_input_padding enable
    - --gemm_plugin float16
    - --gpt_attention_plugin float16 
    - --paged_kv_cache enable
    - --context_fmha enable
    - --output_dir ${TMP_ENGINE_DIR}
    - --max_batch_size 8
    - --use_custom_all_reduce disable
    - '2>&1 | tee $OUTPUT_LOG'
