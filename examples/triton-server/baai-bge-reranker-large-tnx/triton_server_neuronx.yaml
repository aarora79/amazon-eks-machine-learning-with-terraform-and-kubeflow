image: 
  name:
  pull_policy: Always
scheduler_name: neuron-scheduler
resources:
  node_type: inf2.xlarge
  resources:
  requests:
    "aws.amazon.com/neuron": 1
  limits:
    "aws.amazon.com/neuron": 1
tolerations:
  - key: "aws.amazon.com/neuron"
    operator: "Exists"
    effect: "NoSchedule"
inline_script:
- |+
  cat > /tmp/model.py <<EOF
  import json
  import os

  from transformers import AutoModelForSequenceClassification, AutoTokenizer
  import torch
  import logging
  import torch_xla.core.xla_model as xm
  import math
  import numpy as np

  import triton_python_backend_utils as pb_utils

  _MODEL_ARGS_FILENAME = "model.json"

  class TritonPythonModel:

    def initialize(self, args):

      self.model_config = json.loads(args["model_config"])
      scores_config = pb_utils.get_output_config_by_name(self.model_config, "scores")
      self.scores_dtype = pb_utils.triton_string_to_numpy(scores_config["data_type"])

      self.example_pair = ['what is panda?', 
                  'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']
      
      self.load_model()

    @staticmethod
    def auto_complete_config(auto_complete_model_config):
        
      inputs = [
        {"name": "query", "data_type": "TYPE_STRING", "dims": [1]},
        {"name": "texts", "data_type": "TYPE_STRING", "dims": [-1]},
      ]
      outputs = [{"name": "scores", "data_type": "TYPE_FP32", "dims": [-1]}]

      config = auto_complete_model_config.as_dict()
      input_names = []
      output_names = []
      for input in config['input']:
          input_names.append(input['name'])
      for output in config['output']:
          output_names.append(output['name'])

      for input in inputs:
          if input['name'] not in input_names:
              auto_complete_model_config.add_input(input)
      for output in outputs:
          if output['name'] not in output_names:
              auto_complete_model_config.add_output(output)

      auto_complete_model_config.set_model_transaction_policy(dict(decoupled=False))
      auto_complete_model_config.set_max_batch_size(0)

      return auto_complete_model_config
    
    @staticmethod
    def powers_of_2(n):
      return [2**i for i in range(int(math.log2(n))+1)]

    @staticmethod
    def min_power_of_2(n):
      return 2**math.ceil(math.log2(n))

    def _bucket_batch_inference(self, pairs:list) -> list:
      with torch.no_grad():
        inputs = self.tokenizer(pairs, padding="max_length", truncation=True, return_tensors='pt', max_length=512)
        inputs.to(xm.xla_device())
        scores = self.model(**inputs, return_dict=True).logits.view(-1, ).float()
        scores = scores.detach().cpu().numpy().tolist()
        return scores
            
    def run_inference(self, pairs: list):
      batch_size = len(pairs)
      
      bucket_batch_size = min(self.min_power_of_2(batch_size), self.max_model_batch_size)
      pairs.extend([ self.example_pair for _ in range(bucket_batch_size - batch_size) ] )
      scores  = self._bucket_batch_inference(pairs)
      scores = scores[:batch_size]
      return scores
        
    def compile_model(self):
      bucket_list = self.powers_of_2(self.max_model_batch_size)
      for batch_size in bucket_list:
        print(f"Compiling model for batch size: {batch_size}")
        pairs = [ self.example_pair for _ in range(batch_size) ]
        self._bucket_batch_inference(pairs)

    def load_model(self):
    
      self.max_batch_size = int(self.model_config.get('max_batch_size', 0))
      assert (
          self.max_batch_size == 0
      ), "max_batch_size must be set to 0"

      self.using_decoupled = pb_utils.using_decoupled_model_transaction_policy(self.model_config) 
      assert (
          not self.using_decoupled 
      ), "Python backend must be configured to not use decoupled model transaction policy"

      model_args_filepath = os.path.join( 
          pb_utils.get_model_dir(), _MODEL_ARGS_FILENAME
      )
      assert os.path.isfile(
          model_args_filepath
      ), f"'{_MODEL_ARGS_FILENAME}' containing model args must be provided in '{pb_utils.get_model_dir()}'"
      with open(model_args_filepath) as file:
          properties = json.load(file)

      self.max_model_batch_size = properties.get("max_model_batch_size", 8)
      assert (self.max_model_batch_size & (self.max_model_batch_size-1) == 0), \
        "max_model_batch_size must be power of 2"

      assert ( self.max_model_batch_size >= 1 and self.max_model_batch_size <= 16), \
        "max_model_batch_size must be between 1 and 16"

      tokenizer_location = properties.get("tokenizer")
      self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_location)

      model_location = properties.get("model")
      self.model = AutoModelForSequenceClassification.from_pretrained(model_location)

      self.model.eval()
        
      self.model.to(xm.xla_device())
      self.compile_model()

    def execute(self, requests):
      output_dtype = self.scores_dtype

      responses = []
      for request in requests:

        query = pb_utils.get_input_tensor_by_name(request, "query").as_numpy().tolist()
        texts = pb_utils.get_input_tensor_by_name(request, "texts").as_numpy().tolist()

        query_str = query[0].decode("utf-8") if isinstance(query[0], bytes) else query[0]
        pairs = [ [query_str, text.decode("utf-8") if isinstance(text, bytes) else text ] for text in texts ]

        pairs = pairs[:self.max_model_batch_size] if len(pairs) > self.max_model_batch_size else pairs
        scores = self.run_inference(pairs)

        out_tensor = pb_utils.Tensor("scores", np.array(scores).astype(output_dtype))
        inference_response = pb_utils.InferenceResponse(output_tensors=[out_tensor])

        responses.append(inference_response)

      return responses

    def finalize(self):
      print("Cleaning up...")

  EOF

  cat > /tmp/config.pbtxt <<EOF
  backend: "python"
  max_batch_size: 0
  model_transaction_policy {
    decoupled: false
  }

  input [ 
    {
      name: "query"
      data_type: TYPE_STRING
      dims: [1]
    },
    {
      name: "texts"
      data_type: TYPE_STRING
      dims: [-1]
    }
  ] 
  output [
    {
      name: "scores"
      data_type: TYPE_FP32
      dims: [-1]
    }
  ]

  instance_group [
    {
      count: 1
      kind: KIND_MODEL
    }
  ]
  
  EOF
- |+
  cat > /tmp/model.json <<EOF
  {
    "model": "$MODEL_PATH",
    "tokenizer": "$MODEL_PATH",
    "max_model_batch_size": 8
  }

  EOF

pre_script: 
  - mkdir -p $LOG_ROOT
  - OUTPUT_LOG="$LOG_ROOT/triton-server.log"
  - mkdir -p $MODEL_REPO
  - VERSION=1
  - MODEL_NAME=baai-bge-reranker-large
  - mkdir -p $MODEL_REPO/$MODEL_NAME/$VERSION
  - cp /tmp/model.py $MODEL_REPO/$MODEL_NAME/$VERSION/model.py
  - cp /tmp/model.json $MODEL_REPO/$MODEL_NAME/$VERSION/model.json
  - cp /tmp/config.pbtxt $MODEL_REPO/$MODEL_NAME/config.pbtxt
  - export NEURON_CC_FLAGS="--model-type transformer"
  - export NEURON_COMPILE_CACHE_URL="/tmp"
  - export TOKENIZERS_PARALLELISM="false"
server:
  ports:
    http: 8000
    grpc: 8001
    metrics: 8002
  readiness_probe:
    period_secs: 30
    failure_threshold: 10
  startup_probe:
    period_secs: 60
    failure_threshold: 30
  liveness_probe:
    period_secs: 30
    failure_threshold: 10
  env:
    - name: HOME
      value: /tmp
    - name: LOG_ROOT
      value: /efs/home/{{ .Release.Name }}/logs
    - name: MODEL_REPO
      value: /efs/home/{{ .Release.Name }}/model_repository
    - name: MODEL_PATH
      value: "/fsx/pretrained-models/BAAI/bge-reranker-large"
  command:
    - tritonserver
  args:
    - --model-repository=${MODEL_REPO}
    - --grpc-port=8001
    - --http-port=8000
    - --metrics-port=8002
    - --disable-auto-complete-config
    - --log-file=$OUTPUT_LOG
  autoscaling:
    minReplicas: 1
    maxReplicas: 4
    metrics:
      - type: Pods
        pods:
          metric:
            name: avg_time_queue_us
          target:
            type: AverageValue
            averageValue: 50
