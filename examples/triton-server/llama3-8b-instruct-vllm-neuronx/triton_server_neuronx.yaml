image: 
  name:
  pull_policy: Always
scheduler_name: neuron-scheduler
resources:
  node_type: inf2.48xlarge
  resources:
  requests:
    "aws.amazon.com/neuron": 4
    cpu: 16
    memory: 128Gi
  limits:
    "aws.amazon.com/neuron": 4
    cpu: 60
    memory: 180Gi
tolerations:
  - key: "aws.amazon.com/neuron"
    operator: "Exists"
    effect: "NoSchedule"
git:
  repo_url: "https://github.com/vllm-project/vllm.git"
  commit: 5b734fb7edfdf3f8a836a3ddee81eba506230fdd
  branch: main
inline_script:
- |+
  cat > /tmp/config.pbtxt <<EOF
  backend: "vllm"
  max_batch_size: 0
  model_transaction_policy {
    decoupled: true
  }

  input [ 
    {
      name: "text_input"
      data_type: TYPE_STRING
      dims: [1]
    },
    {
        name: "stream"
        data_type: TYPE_BOOL
        dims: [1]
        optional: true
    },
    {
        name: "sampling_parameters"
        data_type: TYPE_STRING
        dims: [1]
        optional: true
    },
    {
        name: "exclude_input_in_output"
        data_type: TYPE_BOOL
        dims: [1]
        optional: true
    }
  ] 
  output [
    {
      name: "text_output"
      data_type: TYPE_STRING
      dims: [-1]
    }
  ]

  instance_group [
    {
      count: 1
      kind: KIND_MODEL
    }
  ]
  
  EOF
- |+
  cat > /tmp/model.json <<EOF
  {
    "model": "$MODEL_PATH",
    "disable_log_requests": true,
    "tensor_parallel_size": 8,
    "max_num_seqs": 4,
    "dtype": "float16",
    "max_model_len": 8192,
    "block_size": 8192
  }

  EOF
- |+
  cat > /tmp/vllm-neuron.patch <<EOF
  diff --git a/vllm/worker/neuron_worker.py b/vllm/worker/neuron_worker.py
  index 9b4367d7..11dbc0f4 100644
  --- a/vllm/worker/neuron_worker.py
  +++ b/vllm/worker/neuron_worker.py
  @@ -68,7 +68,7 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):
          # Set the number of GPU blocks to be the same as the maximum number of
          # sequences that can be processed in a single batch. This is equivalent
          # to schedule without PagedAttention.
  -        num_gpu_blocks = self.scheduler_config.max_num_seqs
  +        num_gpu_blocks = self.scheduler_config.max_num_seqs + 1
  
          # Swap not yet supported with Neuron backend.
          num_cpu_blocks = 0
  @@ -82,7 +82,7 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):
  
          # Different values are not tested.
          assert num_cpu_blocks == 0
  -        assert num_gpu_blocks == self.scheduler_config.max_num_seqs
  +        assert num_gpu_blocks == self.scheduler_config.max_num_seqs + 1
  
          self.cache_config.num_gpu_blocks = num_gpu_blocks
          self.cache_config.num_cpu_blocks = num_cpu_blocks

  EOF
pre_script: 
  - mkdir -p $LOG_ROOT
  - OUTPUT_LOG="$LOG_ROOT/triton_server.log"
  - rm -rf $MODEL_REPO
  - mkdir -p $MODEL_REPO
  - VERSION=1
  - MODEL_NAME=llama3-8b-instruct
  - mkdir -p $MODEL_REPO/$MODEL_NAME/$VERSION
  - cp /tmp/model.json $MODEL_REPO/$MODEL_NAME/$VERSION/model.json
  - cp /tmp/config.pbtxt $MODEL_REPO/$MODEL_NAME/config.pbtxt
  - git apply --ignore-whitespace /tmp/vllm-neuron.patch
  - pip3 install -r requirements-neuron.txt 
  - pip3 install .
  - pip3 install triton==2.2.0
  - pip3 install pynvml==11.5.3
  - git clone https://github.com/triton-inference-server/vllm_backend.git
  - cd vllm_backend 
  - git fetch origin 507e4dccabf85c3b7821843261bcea7ea5828802
  - git reset --hard 507e4dccabf85c3b7821843261bcea7ea5828802
  - mkdir -p /opt/tritonserver/backends/vllm
  - cp -r src/* /opt/tritonserver/backends/vllm/
  - cd $GIT_CLONE_DIR
  - export NEURON_CC_FLAGS="--model-type transformer"
  - export NEURON_COMPILE_CACHE_URL="/tmp"
  - export OMP_NUM_THREADS=32
server:
  ports:
    http: 8000
    grpc: 8001
    metrics: 8002
  readiness_probe:
    period_secs: 30
    failure_threshold: 10
  startup_probe:
    period_secs: 60
    failure_threshold: 30
  liveness_probe:
    period_secs: 30
    failure_threshold: 10
  env:
    - name: HOME
      value: /tmp
    - name: LOG_ROOT
      value: /efs/home/{{ .Release.Name }}/logs
    - name: MODEL_REPO
      value: /efs/home/{{ .Release.Name }}/model_repository
    - name: MODEL_PATH
      value: "/fsx/pretrained-models/meta-llama/Meta-Llama-3-8B-Instruct"
  command:
    - tritonserver
  args:
    - --model-repository=${MODEL_REPO}
    - --grpc-port=8001
    - --http-port=8000
    - --metrics-port=8002
    - --disable-auto-complete-config
    - --log-file=$OUTPUT_LOG
  autoscaling:
    minReplicas: 3
    maxReplicas: 6
    metrics:
      - type: Pods
        pods:
          metric:
            name: avg_time_queue_us
          target:
            type: AverageValue
            averageValue: 1000000
