apiVersion: v1
kind: ConfigMap
metadata:
  name: rayserve-tnx-autocausal-{{ .Release.Name }}
data:
  rayserve-tnx-autocausal.sh: |
    #!/bin/bash

    apt-get update
    apt-get install -y zip 

    cat > /tmp/tnx_autocausal.py <<EOF
    import os
    import json

    from fastapi import FastAPI
    from ray import serve

    import torch
    from transformers_neuronx import NeuronAutoModelForCausalLM
    from transformers import AutoTokenizer
    
    app = FastAPI()

    @serve.deployment()
    @serve.ingress(app)
    class TNXAutoCausalDeployment:

      def __init__(self):
     
        config = os.getenv("ENGINE_CONFIG")
        assert config, "ENGINE_CONFIG env variable for engine config path is required"
        assert os.path.isfile(config), f"ENGINE_CONFIG {config} does not exist"

        # Load engine args from config file
        with open(config, "r") as f:
          engine_config = json.load(f)

        num_cores = int(engine_config.get("tp_degree", 1))
        base_index = int(os.getenv("BASE_INDEX", 0))
        base_core = base_index * num_cores
        os.environ["NEURON_RT_VISIBLE_CORES"]  = ",".join(map(str, list(range(base_core,num_cores+base_core))))
   
        pretrained_model_name_or_path = engine_config.get("pretrained_model_name_or_path")
        assert pretrained_model_name_or_path, "pretrained_model_name_or_path is required in engine config"
        assert os.path.isdir(pretrained_model_name_or_path), f"model {pretrained_model_name_or_path} does not exist"

        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)
        self.model = NeuronAutoModelForCausalLM.from_pretrained(**engine_config)
        self.model.to_neuron()

      @app.post("/generate")
      def generate(self, prompt: str, **sampling_params) -> str:
        with torch.inference_mode():
          input_ids = self.tokenizer.encode_plus(prompt, return_tensors="pt")['input_ids']
          generated_token_seqs = self.model.sample(input_ids, **sampling_params)
          text_output_seqs = [self.tokenizer.decode(seq, skip_special_tokens=True) for seq in generated_token_seqs]

        return text_output_seqs[0]

    deployment = TNXAutoCausalDeployment.bind() 

    EOF

    cd /tmp
    if [ -z "${ENGINE_PATH}" ]; then  echo "ENGINE_PATH env variable is required" && exit 1; fi
    ENGINE_DIR="$(dirname "${ENGINE_PATH}")"
    mkdir -p $ENGINE_DIR
    zip $ENGINE_PATH tnx_autocausal.py
---
apiVersion: v1
kind: Pod
metadata:
  name: rayserve-tnx-autocausal-{{ .Release.Name }}
  annotations:
    karpenter.sh/do-not-disrupt: "true"
    sidecar.istio.io/inject: 'false'
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  restartPolicy: Never
  volumes:
  {{- $pv_index := 1 }}
  {{- range $pv := .Values.pvc }}
  - name: pv-{{ $pv_index }}
    persistentVolumeClaim:
      claimName: {{ $pv.name }}
  {{- $pv_index = add $pv_index 1 }}
  {{- end }}
  - name: config
    configMap:
      defaultMode: 420
      items:
      - key: rayserve-tnx-autocausal.sh
        mode: 365
        path: rayserve-tnx-autocausal.sh
      name: rayserve-tnx-autocausal-{{ .Release.Name }}
  containers:
  - name: rayserve-tnx-autocausal
    env:
    - name: ENGINE_PATH
      value: {{ .Values.engine_path }}
    {{- range $v := .Values.env }}
    - name: {{ $v.name }}
      value: "{{ tpl $v.value $ }}"
    {{- end }}
    command:
    -  sh 
    - /etc/config/rayserve-tnx-autocausal.sh
    image: public.ecr.aws/docker/library/python:slim-bullseye
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - mountPath: /etc/config
      name: config
    {{- $pv_index := 1 }}
    {{- range $pv := .Values.pvc }}
    - mountPath: {{ $pv.mount_path }}
      name: pv-{{ $pv_index }}
    {{- $pv_index = add $pv_index 1 }}
    {{- end }}
    resources:
      requests:
      {{- range $k, $v := .Values.resources.requests }}
        {{ $k }}: {{ $v }}
      {{- end }}
      limits:
      {{- range $k, $v := .Values.resources.limits }}
        {{ $k }}: {{ $v }}
      {{- end }}
