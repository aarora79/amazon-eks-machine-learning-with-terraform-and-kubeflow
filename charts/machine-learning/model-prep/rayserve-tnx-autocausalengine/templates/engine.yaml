apiVersion: v1
kind: ConfigMap
metadata:
  name: rayserve-tnx-autocausal-{{ .Release.Name }}
data:
  rayserve-tnx-autocausal.sh: |
    #!/bin/bash

    apt-get update
    apt-get install -y zip 

    cat > /tmp/tnx_autocausal.py <<EOF
    import os
    import json

    from fastapi import FastAPI, Request
    from fastapi.responses import JSONResponse, Response
    from ray import serve

    import torch
    from transformers_neuronx import NeuronAutoModelForCausalLM
    from transformers import AutoTokenizer
    from transformers_neuronx.config import NeuronConfig,  GenerationConfig, QuantizationConfig, ContinuousBatchingConfig
    
    app = FastAPI()

    @serve.deployment()
    @serve.ingress(app)
    class TNXAutoCausalDeployment:

      def __init__(self):
     
        config = os.getenv("ENGINE_CONFIG")
        assert config, "ENGINE_CONFIG env variable for engine config path is required"
        assert os.path.isfile(config), f"ENGINE_CONFIG {config} does not exist"

        master_addr = os.getenv("FQ_RAY_IP", None)
        if master_addr is not None:
          os.environ["MASTER_ADDR"] = f"{master_addr}"
          master_port = os.getenv("MASTER_PORT", None)
          if master_port is None:
            master_port = 43000
            os.environ["MASTER_PORT"] = f"{master_port}"
          os.environ["CPU_COMM_ID"] = "{master_addr}:{master_port}"

        # Load engine args from config file
        with open(config, "r") as f:
          engine_config = json.load(f)

        pretrained_model_name_or_path = engine_config.pop("pretrained_model_name_or_path")
        assert pretrained_model_name_or_path, "pretrained_model_name_or_path is required in engine config"
        assert os.path.isdir(pretrained_model_name_or_path), f"model {pretrained_model_name_or_path} does not exist"

        neuron_config_dict = engine_config.pop("neuron_config", {})
        continuous_batching = neuron_config_dict.pop("continuous_batching", {})
        if continuous_batching:
          if "max_num_seqs" in continuous_batching:
            engine_config["batch_size"] = int(continuous_batching.get("max_num_seqs"))
          elif "batch_size_for_shared_caches" in continuous_batching:
            engine_config["batch_size"] = int(continuous_batching.get("batch_size_for_shared_caches"))
          neuron_config_dict["continuous_batching"] = ContinuousBatchingConfig(**continuous_batching)
         
        quant = neuron_config_dict.pop("quant", {})
        if "quant_dtype" in quant and "dequant_dtype" in quant:
          neuron_config_dict["quant"] = QuantizationConfig(**quant)
        
        on_device_generation = neuron_config_dict.pop("on_device_generation", {})
        if on_device_generation:
          neuron_config_dict["on_device_generation"] = GenerationConfig(**on_device_generation)

        neuron_config = None
        if neuron_config_dict:
          engine_config["neuron_config"] = NeuronConfig(**neuron_config_dict)

        self.batch_size = engine_config.get("batch_size", 1)
        self.n_positions = engine_config.get("n_positions", 2048)
        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)

        print(f"Normalized engine_config: {engine_config}")
        self.model = NeuronAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **engine_config)
        print("Move model to neuron device")
        self.model.to_neuron()
        print("Moved model to neuron device")

      @app.post("/generate")
      async def generate(self, request: Request) -> Response:
        """Generate completion for the request.
            The request should be a JSON object with the following fields:
            - prompts: the prompts to use for the generation.
            - sampling_params: the sampling parameters.
        """
        request_dict = await request.json()
        prompts = request_dict.pop("prompt")
        sampling_params = request_dict.pop("sampling_params", {})
        assert isinstance(sampling_params, dict)

        if "sequence_length" not in sampling_params:
          sampling_params["sequence_length"] = self.n_positions
          
        prompts = [prompts] if isinstance(prompts, str) else prompts
        assert isinstance(prompts, list)

        n_prompts = len(prompts)
        prompts.extend([ prompts[-1] for _ in range(self.batch_size - n_prompts)])
        with torch.inference_mode():
          input_ids = self.tokenizer.batch_encode_plus(prompts, return_tensors="pt")['input_ids']
          generated_token_seqs = self.model.sample(input_ids, **sampling_params)
          generated_token_seqs = generated_token_seqs[:n_prompts]
          generated_text_seqs = self.tokenizer.batch_decode(generated_token_seqs, skip_special_tokens=True)
        
        ret = {"text_outputs": generated_text_seqs}
        return JSONResponse(ret)

    deployment = TNXAutoCausalDeployment.bind() 

    EOF

    cd /tmp
    if [ -z "${ENGINE_PATH}" ]; then  echo "ENGINE_PATH env variable is required" && exit 1; fi
    ENGINE_DIR="$(dirname "${ENGINE_PATH}")"
    mkdir -p $ENGINE_DIR
    zip $ENGINE_PATH tnx_autocausal.py
---
apiVersion: v1
kind: Pod
metadata:
  name: rayserve-tnx-autocausal-{{ .Release.Name }}
  annotations:
    karpenter.sh/do-not-disrupt: "true"
    sidecar.istio.io/inject: 'false'
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  restartPolicy: Never
  volumes:
  {{- $pv_index := 1 }}
  {{- range $pv := .Values.pvc }}
  - name: pv-{{ $pv_index }}
    persistentVolumeClaim:
      claimName: {{ $pv.name }}
  {{- $pv_index = add $pv_index 1 }}
  {{- end }}
  - name: config
    configMap:
      defaultMode: 420
      items:
      - key: rayserve-tnx-autocausal.sh
        mode: 365
        path: rayserve-tnx-autocausal.sh
      name: rayserve-tnx-autocausal-{{ .Release.Name }}
  containers:
  - name: rayserve-tnx-autocausal
    env:
    - name: ENGINE_PATH
      value: {{ .Values.engine_path }}
    {{- range $v := .Values.env }}
    - name: {{ $v.name }}
      value: "{{ tpl $v.value $ }}"
    {{- end }}
    command:
    -  sh 
    - /etc/config/rayserve-tnx-autocausal.sh
    image: public.ecr.aws/docker/library/python:slim-bullseye
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - mountPath: /etc/config
      name: config
    {{- $pv_index := 1 }}
    {{- range $pv := .Values.pvc }}
    - mountPath: {{ $pv.mount_path }}
      name: pv-{{ $pv_index }}
    {{- $pv_index = add $pv_index 1 }}
    {{- end }}
    resources:
      requests:
      {{- range $k, $v := .Values.resources.requests }}
        {{ $k }}: {{ $v }}
      {{- end }}
      limits:
      {{- range $k, $v := .Values.resources.limits }}
        {{ $k }}: {{ $v }}
      {{- end }}
