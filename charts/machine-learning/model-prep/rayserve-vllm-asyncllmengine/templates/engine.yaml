apiVersion: v1
kind: ConfigMap
metadata:
  name: rayserve-vllm-asyncllmengine-{{ .Release.Name }}
data:
  rayserve-vllm-asyncllmengine.sh: |
    #!/bin/bash

    apt-get update
    apt-get install -y zip 
    
    cat > /tmp/vllm_asyncllmengine.py <<EOF
    import os
    import json
    from typing import AsyncGenerator
    import asyncio

    from fastapi import FastAPI, Request
    from fastapi.responses import JSONResponse, Response, StreamingResponse
    from ray import serve

    from vllm.engine.arg_utils import AsyncEngineArgs
    from vllm.engine.async_llm_engine import AsyncLLMEngine
    from vllm.sampling_params import SamplingParams
    from vllm.utils import random_uuid

    app = FastAPI()

    @serve.deployment()
    @serve.ingress(app)
    class VLLMDeployment:
      def __init__(self):

        config = os.getenv("ENGINE_CONFIG")
        assert config, "ENGINE_CONFIG env variable for engine config path is required"
        assert os.path.isfile(config), f"ENGINE_CONFIG {config} does not exist"

        # Load engine args from config file
        with open(config, "r") as f:
          engine_config = json.load(f)

        engine_args = AsyncEngineArgs(**engine_config)
        self.engine = AsyncLLMEngine.from_engine_args(engine_args)

      @app.post("/generate")
      async def generate(self, request: Request) -> Response:
        """Generate completion for the request.
        The request should be a JSON object with the following fields:
        - prompt: the prompt to use for the generation.
        - stream: whether to stream the results or not.
        - sampling_params: the sampling parameters (See `SamplingParams` for details).
        """
        request_dict = await request.json()
        prompt = request_dict.pop("prompt")
        stream = request_dict.pop("stream", False)
        sampling_params_dict = request_dict.pop("sampling_params", {})
        sampling_params = SamplingParams(**sampling_params_dict)
        request_id = random_uuid()

        results_generator = self.engine.generate(prompt, sampling_params, request_id)
        
        # Streaming case
        async def stream_results() -> AsyncGenerator[bytes, None]:
          async for request_output in results_generator:
              prompt = request_output.prompt
              text_outputs = [
                  prompt + output.text for output in request_output.outputs
              ]
              ret = {"text": text_outputs}
              yield (json.dumps(ret) + "\0").encode("utf-8")

        if stream:
          return StreamingResponse(stream_results())

        # Non-streaming case
        final_output = None
        try:
          async for request_output in results_generator:
            final_output = request_output
        except asyncio.CancelledError:
            return Response(status_code=499)

        assert final_output is not None
        prompt = final_output.prompt
        text_outputs = [prompt + output.text for output in final_output.outputs]
        ret = {"text_outputs": text_outputs}
        return JSONResponse(ret)
    
    deployment = VLLMDeployment.bind()

    EOF

    cd /tmp
    if [ -z "${ENGINE_PATH}" ]; then  echo "ENGINE_PATH env variable is required" && exit 1; fi
    ENGINE_DIR="$(dirname "${ENGINE_PATH}")"
    mkdir -p $ENGINE_DIR
    zip $ENGINE_PATH vllm_asyncllmengine.py
---
apiVersion: v1
kind: Pod
metadata:
  name: rayserve-vllm-asyncllmengine-{{ .Release.Name }}
  annotations:
    karpenter.sh/do-not-disrupt: "true"
    sidecar.istio.io/inject: 'false'
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  restartPolicy: Never
  volumes:
  {{- $pv_index := 1 }}
  {{- range $pv := .Values.pvc }}
  - name: pv-{{ $pv_index }}
    persistentVolumeClaim:
      claimName: {{ $pv.name }}
  {{- $pv_index = add $pv_index 1 }}
  {{- end }}
  - name: config
    configMap:
      defaultMode: 420
      items:
      - key: rayserve-vllm-asyncllmengine.sh
        mode: 365
        path: rayserve-vllm-asyncllmengine.sh
      name: rayserve-vllm-asyncllmengine-{{ .Release.Name }}
  containers:
  - name: rayserve-vllm-asyncllmengine
    env:
    - name: ENGINE_PATH
      value: {{ .Values.engine_path }}
    {{- range $v := .Values.env }}
    - name: {{ $v.name }}
      value: "{{ tpl $v.value $ }}"
    {{- end }}
    command:
    -  sh 
    - /etc/config/rayserve-vllm-asyncllmengine.sh
    image: public.ecr.aws/docker/library/python:slim-bullseye
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - mountPath: /etc/config
      name: config
    {{- $pv_index := 1 }}
    {{- range $pv := .Values.pvc }}
    - mountPath: {{ $pv.mount_path }}
      name: pv-{{ $pv_index }}
    {{- $pv_index = add $pv_index 1 }}
    {{- end }}
    resources:
      requests:
      {{- range $k, $v := .Values.resources.requests }}
        {{ $k }}: {{ $v }}
      {{- end }}
      limits:
      {{- range $k, $v := .Values.resources.limits }}
        {{ $k }}: {{ $v }}
      {{- end }}
